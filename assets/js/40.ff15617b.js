(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{367:function(t,a,s){t.exports=s.p+"assets/img/seamask.7825e940.png"},368:function(t,a,s){t.exports=s.p+"assets/img/dashboard.c1519458.png"},369:function(t,a,s){t.exports=s.p+"assets/img/dashboard_with_raster.3d92dc9a.png"},598:function(t,a,s){"use strict";s.r(a);var e=s(4),n=Object(e.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"vessel-detection"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vessel-detection"}},[t._v("#")]),t._v(" Vessel Detection")]),t._v(" "),a("p",[t._v("In this notebook we will learn how to apply adaptive thresholding to SENTINEL1_GRD data within OpenEO Platform. We will then take a look at our results against vessel location data from the Maritime Traffic Agency over the Adriatic. We will be using the the openeo-python-client to prepare our process graph, and a Plotly Dash dashboard to interact with our results.")]),t._v(" "),a("h2",{attrs:{id:"adaptive-thresholding-in-xarray"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#adaptive-thresholding-in-xarray"}},[t._v("#")]),t._v(" Adaptive Thresholding in Xarray")]),t._v(" "),a("p",[t._v("After investigating the code initially provided by Planetek, we arrived at this xarray implementation of adaptive thresholding.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("WINDOW_LAT_SIZE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nWINDOW_LON_SIZE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nTHRESHOLD_FACT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\nout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" xr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones_like"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nrolling_mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rolling"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    longitude"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("WINDOW_LAT_SIZE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" latitude"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("WINDOW_LON_SIZE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" center"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nthresholded_image "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" rolling_mean "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" THRESHOLD_FACT\nraster "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("where"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("thresholded_image "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" other"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("We are going to port this into an OpenEO process graph.")]),t._v(" "),a("h3",{attrs:{id:"_1-authenticate"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-authenticate"}},[t._v("#")]),t._v(" 1. Authenticate")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" openeo\n\nbackend "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"openeo.cloud"')]),t._v("\nconn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" openeo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("connect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("backend"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("authenticate_oidc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"_2-pre-processing-prep"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-pre-processing-prep"}},[t._v("#")]),t._v(" 2. Pre-processing prep")]),t._v(" "),a("p",[t._v("This process graph requires either a sea or land mask to remove the land from our AOI. A seamask has been prepared for this example and is available on github. It was derived using the "),a("a",{attrs:{href:"https://land.copernicus.eu/content/corine-land-cover-nomenclature-guidelines/html/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Water Bodies"),a("OutboundLink")],1),t._v(" classes from Corine Land Cover.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" folium\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We are going to use the following GeoJson to operate as a sea mask for out process graph.")]),t._v("\nSEA_MASK "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://raw.githubusercontent.com/SerRichard/sea_mask/main/sea-mask-4326.json"')]),t._v("\n\nfig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" folium"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Figure"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("600")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" folium"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("location"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("44.465488")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12.602316")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zoom_start"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfolium"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GeoJson"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SEA_MASK"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_child"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("img",{attrs:{src:s(367),alt:"SeaMask to be used"}})]),t._v(" "),a("h3",{attrs:{id:"_3-defining-a-process-graph-to-detect-vessels-in-our-aoi"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-defining-a-process-graph-to-detect-vessels-in-our-aoi"}},[t._v("#")]),t._v(" 3. Defining a process graph to detect vessels in our AOI.")]),t._v(" "),a("p",[t._v("The spatial_extent we have defined includes, but is not limited to the sea mask that we will be using. We will use both available polarizations from the SENTINEL1_GRD collection, and run this graph on a little over a weeks worth of data.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("spatial_extent  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"west"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12.194377989297493")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"east"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12.758093271633888")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"south"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("44.24420099164355")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"north"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("44.85455845353388")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ntemporal_extent "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2021-10-01"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2021-10-09"')]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\ns1_datacube "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_collection"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"SENTINEL1_GRD"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    spatial_extent"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("spatial_extent"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    bands"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"VV"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"VH"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    temporal_extent"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("temporal_extent\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("Load the geometries as a vector cube. This is currently a custom process, so is not available at all backend. Could be replaced with load_url if this is an issue.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("sea_mask "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" s1_datacube"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"load_vector_cube"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"URL"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" SEA_MASK"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("Replace the values that lie outside of our polygon with NaN values.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("masked_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" s1_datacube"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mask_polygon"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sea_mask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("To achieve the equivalent functionality of "),a("code",[t._v("data.rolling()")]),t._v(", we will use the process apply_kernel. The kernel will be the length and width of the window we want to apply the convolution to. The value of each pixel in the kernel is equal to "),a("code",[t._v("1/(kernel_width*kernel_height)")]),t._v(", spreading the weight evenly will achieve a mean for across the pixels in the kernel. The resut of the convolution is multiplied then multipled in the process by the factor value.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("kernel_value "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.00104")]),t._v("\nkernel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n      kernel_value "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" y "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("31")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("31")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\napplied_kernel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" masked_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apply_kernel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("kernel"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("kernel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("factor"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("Next we want to compare the results of apply_kernel, against the values of the data we initially masked. We can use the process merge_cubes with the overlap resolver set to the less than process to acheive this.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("lt_comparison "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" applied_kernel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("merge_cubes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  masked_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" overlap_resolver"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lt"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("Convert the resulting boolean datacube to a vector cube, and output the result in save result as a GeoJson. This will make the comparison of our results with the Maritime traffic data more straight forward.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("output_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lt_comparison"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("raster_to_vector"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("vessel_detection "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" output_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"GeoJSON"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"_4-run-auxilliary-job"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-run-auxilliary-job"}},[t._v("#")]),t._v(" 4. Run Auxilliary job")]),t._v(" "),a("p",[t._v("You can optionally run the following job, if you would like to compare the original Sentinel1_GRD data, against the results from the vessel detection. This comparison makes it easier to visually validate the results, and spot erroneous polygons.")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('sentinel1_data = s1_datacube.save_result(format="GTiff")\nsentinel1_data_job = sentinel1_data.create_job(title = "UC2-Auxilliary-Job")\nsentinel1_data_job.start_job()\n')])])]),a("h2",{attrs:{id:"result-visualisation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#result-visualisation"}},[t._v("#")]),t._v(" Result Visualisation")]),t._v(" "),a("p",[t._v("A small dashboard has been provided with the Plotly Dash library to quickly visualise the results of our processing. We're using this dashboard so we can interface with the PyGeoApi server that hosts the Maritime Traffic data. You will need to use the canonical url of the job result, this can be found in the notebook via a helper fuinction, or in the job information via the web editor.")]),t._v(" "),a("p",[t._v("PyGeoApi Data: https://features.dev.services.eodc.eu/collections/adriatic_vessels")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" eodc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("visualisation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vessel_detection"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("app "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" app\napp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h4",{attrs:{id:"plotly-dashboard-for-results-viewing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#plotly-dashboard-for-results-viewing"}},[t._v("#")]),t._v(" Plotly Dashboard for results viewing")]),t._v(" "),a("p",[a("img",{attrs:{src:s(368),alt:"Plotly Dashboard"}})]),t._v(" "),a("h4",{attrs:{id:"with-the-additional-auxilliary-job"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#with-the-additional-auxilliary-job"}},[t._v("#")]),t._v(" With the additional auxilliary job")]),t._v(" "),a("p",[a("img",{attrs:{src:s(369),alt:"Plotly Dashboard"}})]),t._v(" "),a("h2",{attrs:{id:"use-case-recap"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#use-case-recap"}},[t._v("#")]),t._v(" Use Case -- Recap")]),t._v(" "),a("p",[t._v("We've been through a number of iterations to arrive at this implementation.")]),t._v(" "),a("ol",[a("li",[t._v('Recieved an initial implementation from Planetek, which we packaged and released as a custom function, "vessel_detection".')]),t._v(" "),a("li",[t._v('Porting to Xarray. This resulted in a single function called "adaptive_thresholding", that removed the dependency to the ellipsoid corrected SENTINEL1_GRD imagery.')]),t._v(" "),a("li",[t._v("Porting to existing OpenEO processes. This addresses the feedback from the previous review, i.e. the implementation should be reproducable for this use case.")])])])}),[],!1,null,null,null);a.default=n.exports}}]);